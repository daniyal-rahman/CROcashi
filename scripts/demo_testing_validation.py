#!/usr/bin/env python3
"""
Demo script for Phase 5: Testing & Validation framework.

This script demonstrates the comprehensive testing and validation capabilities
including synthetic data generation, performance benchmarking, validation
frameworks, and edge case testing.
"""

import sys
import time
from datetime import datetime
from typing import Dict, Any, List

# Add src to path for imports
sys.path.insert(0, 'src')

from ncfd.testing import (
    SyntheticDataGenerator, create_test_scenarios,
    PerformanceBenchmark, ValidationFramework, EdgeCaseValidator
)


def demo_synthetic_data_generation():
    """Demonstrate synthetic data generation capabilities."""
    print("üß¨ DEMO: SYNTHETIC DATA GENERATION")
    print("=" * 50)
    
    generator = SyntheticDataGenerator(seed=42)
    
    # Generate study cards with different scenarios
    scenarios = create_test_scenarios()
    
    print(f"üìä Generated {len(scenarios)} predefined test scenarios:")
    for i, scenario in enumerate(scenarios[:5], 1):  # Show first 5
        print(f"  {i}. {scenario.name}")
        print(f"     Type: {scenario.trial_type.value}, Indication: {scenario.indication.value}")
        print(f"     Failure modes: {len(scenario.failure_modes)}, Expected risk: {scenario.expected_risk_level}")
        
        # Generate study card for this scenario
        study_card = generator.generate_study_card(scenario)
        print(f"     Study ID: {study_card['study_id']}, Pivotal: {study_card['is_pivotal']}")
        print()
    
    # Generate trial versions
    print("üìà Generating trial version history...")
    base_study_card = generator.generate_study_card()
    versions = generator.generate_trial_versions(base_study_card, num_versions=4)
    
    print(f"Generated {len(versions)} versions for trial {base_study_card['study_id']}:")
    for version in versions:
        print(f"  Version {version['version_id']}: captured {version['captured_at'].strftime('%Y-%m-%d')}")
        print(f"    Sample size: {version['sample_size']}, Changes: {len(version['changes_jsonb'].get('change_summary', []))}")
    
    # Generate historical data for calibration
    print("\nüèõÔ∏è  Generating historical trial data...")
    historical_data = generator.generate_historical_data(num_trials=50)
    
    # Analyze historical data
    outcomes = [trial["actual_outcome"] for trial in historical_data]
    failure_rate = sum(outcomes) / len(outcomes)
    
    gate_firings = {}
    for trial in historical_data:
        for gate_id in trial["gates_fired"]:
            gate_firings[gate_id] = gate_firings.get(gate_id, 0) + 1
    
    print(f"Generated {len(historical_data)} historical trials:")
    print(f"  Overall failure rate: {failure_rate:.1%}")
    print(f"  Gate firing frequencies:")
    for gate_id in ["G1", "G2", "G3", "G4"]:
        count = gate_firings.get(gate_id, 0)
        print(f"    {gate_id}: {count} times ({count/len(historical_data):.1%})")
    
    return historical_data


def demo_performance_benchmarking():
    """Demonstrate performance benchmarking capabilities."""
    print("\n‚ö° DEMO: PERFORMANCE BENCHMARKING")
    print("=" * 50)
    
    benchmark = PerformanceBenchmark(warmup_iterations=5)
    
    # Quick benchmark on smaller datasets for demo
    print("üèÉ Running quick performance benchmarks...")
    
    sizes = [10, 25, 50]  # Smaller sizes for demo
    
    for size in sizes:
        print(f"\nüìä Benchmarking with {size} trials:")
        
        # Benchmark signals
        signal_metric = benchmark.benchmark_signal_evaluation(size)
        print(f"  Signals: {signal_metric.items_per_second:.0f} trials/sec, "
              f"{signal_metric.avg_time_per_item*1000:.1f}ms/trial")
        
        # Benchmark full pipeline
        pipeline_metric = benchmark.benchmark_full_pipeline(size)
        print(f"  Pipeline: {pipeline_metric.items_per_second:.0f} trials/sec, "
              f"{pipeline_metric.avg_time_per_item*1000:.1f}ms/trial")
        
        print(f"  Memory: {signal_metric.memory_peak_mb:.1f}MB peak, "
              f"{signal_metric.memory_delta_mb:.1f}MB delta")
    
    # Memory usage analysis
    print("\nüß† Memory usage analysis...")
    memory_analysis = benchmark.benchmark_memory_usage(max_trials=1000)
    
    memory_measurements = memory_analysis["memory_measurements"]
    print(f"Memory scaling across {len(memory_measurements)} test sizes:")
    for measurement in memory_measurements[-3:]:  # Show last 3
        size = measurement["size"]
        mb_per_trial = measurement["mb_per_trial"] * 1024  # Convert to KB
        print(f"  {size:4d} trials: {mb_per_trial:.1f}KB/trial")
    
    scaling_analysis = memory_analysis["memory_scaling_analysis"]
    print(f"Memory efficiency: {scaling_analysis['memory_efficiency']}")
    print(f"Average memory per trial: {scaling_analysis['avg_mb_per_trial']*1024:.1f}KB")


def demo_validation_framework():
    """Demonstrate validation framework capabilities."""
    print("\nüîç DEMO: VALIDATION FRAMEWORK")
    print("=" * 50)
    
    framework = ValidationFramework(random_seed=42)
    
    # Run validation on a smaller dataset for demo
    print("üß™ Running validation with synthetic data...")
    
    # Generate validation dataset
    validation_data = framework._generate_validation_dataset(100)  # Smaller for demo
    
    print(f"Generated {len(validation_data)} validation trials")
    
    # Show scenario distribution
    scenario_counts = {}
    for trial in validation_data:
        scenario_name = trial["scenario"].name
        scenario_counts[scenario_name] = scenario_counts.get(scenario_name, 0) + 1
    
    print(f"Scenario distribution (top 5):")
    for scenario, count in sorted(scenario_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"  {scenario}: {count} trials")
    
    # Validate signal accuracy (subset for demo)
    print(f"\nüéØ Validating signal accuracy...")
    signal_metrics = framework.validate_signal_accuracy(validation_data[:50])  # Subset for speed
    
    print("Signal performance:")
    for metric in signal_metrics[:5]:  # Show first 5 signals
        signal_id = metric.component_name.split("_")[1]
        print(f"  {signal_id}: Acc={metric.accuracy:.3f}, Prec={metric.precision:.3f}, "
              f"Rec={metric.recall:.3f}, F1={metric.f1_score:.3f}")
    
    # Validate gate logic
    print(f"\nüö™ Validating gate logic...")
    gate_metrics = framework.validate_gate_logic(validation_data[:50])
    
    print("Gate performance:")
    for metric in gate_metrics:
        gate_id = metric.component_name.split("_")[1]
        print(f"  {gate_id}: Acc={metric.accuracy:.3f}, Prec={metric.precision:.3f}, "
              f"Rec={metric.recall:.3f}, F1={metric.f1_score:.3f}")
    
    # Validate scoring accuracy
    print(f"\nüéØ Validating scoring accuracy...")
    scoring_metric = framework.validate_scoring_accuracy(validation_data[:50])
    
    print("Scoring performance:")
    print(f"  Accuracy: {scoring_metric.accuracy:.3f}")
    print(f"  Precision: {scoring_metric.precision:.3f}")
    print(f"  Recall: {scoring_metric.recall:.3f}")
    print(f"  F1-Score: {scoring_metric.f1_score:.3f}")
    if scoring_metric.auc_score:
        print(f"  AUC: {scoring_metric.auc_score:.3f}")
    
    return validation_data


def demo_edge_case_testing():
    """Demonstrate edge case testing capabilities."""
    print("\nüß™ DEMO: EDGE CASE TESTING")
    print("=" * 50)
    
    validator = EdgeCaseValidator()
    
    print("üî¨ Running comprehensive edge case tests...")
    
    # Test missing data scenarios
    print("\nüìã Testing missing data scenarios...")
    missing_data_results = validator.test_missing_data_scenarios()
    
    passed = sum(1 for r in missing_data_results if r.passed)
    total = len(missing_data_results)
    print(f"Missing data tests: {passed}/{total} passed ({passed/total*100:.1f}%)")
    
    # Show a few specific results
    for result in missing_data_results[:3]:
        status = "‚úÖ" if result.passed else "‚ùå"
        print(f"  {status} {result.test_name}: {result.actual_behavior}")
    
    # Test extreme values
    print("\nüî¢ Testing extreme values...")
    extreme_results = validator.test_extreme_values()
    
    passed = sum(1 for r in extreme_results if r.passed)
    total = len(extreme_results)
    print(f"Extreme value tests: {passed}/{total} passed ({passed/total*100:.1f}%)")
    
    # Show results for extreme p-values
    p_value_tests = [r for r in extreme_results if "P-value" in r.test_name]
    for result in p_value_tests[:3]:
        status = "‚úÖ" if result.passed else "‚ùå"
        print(f"  {status} {result.test_name}: {result.actual_behavior}")
    
    # Test boundary conditions
    print("\nüéØ Testing boundary conditions...")
    boundary_results = validator.test_boundary_conditions()
    
    passed = sum(1 for r in boundary_results if r.passed)
    total = len(boundary_results)
    print(f"Boundary condition tests: {passed}/{total} passed ({passed/total*100:.1f}%)")
    
    # Show S8 boundary tests
    s8_tests = [r for r in boundary_results if "S8" in r.test_name]
    for result in s8_tests:
        status = "‚úÖ" if result.passed else "‚ùå"
        print(f"  {status} {result.test_name}: {result.actual_behavior}")
    
    # Test error handling
    print("\n‚ö†Ô∏è  Testing error handling...")
    error_results = validator.test_error_handling()
    
    passed = sum(1 for r in error_results if r.passed)
    total = len(error_results)
    print(f"Error handling tests: {passed}/{total} passed ({passed/total*100:.1f}%)")
    
    for result in error_results[:2]:
        status = "‚úÖ" if result.passed else "‚ùå"
        print(f"  {status} {result.test_name}: {result.actual_behavior}")
    
    # Overall edge case summary
    all_results = missing_data_results + extreme_results + boundary_results + error_results
    total_passed = sum(1 for r in all_results if r.passed)
    total_tests = len(all_results)
    
    print(f"\nüìä Overall edge case testing:")
    print(f"Total tests: {total_tests}")
    print(f"Passed: {total_passed} ({total_passed/total_tests*100:.1f}%)")
    print(f"Failed: {total_tests - total_passed} ({(total_tests - total_passed)/total_tests*100:.1f}%)")
    
    return all_results


def demo_integration_testing():
    """Demonstrate integration testing across all components."""
    print("\nüîÑ DEMO: INTEGRATION TESTING")
    print("=" * 50)
    
    print("üß™ Testing complete pipeline integration...")
    
    # Generate test scenarios
    generator = SyntheticDataGenerator(seed=123)
    scenarios = create_test_scenarios()
    
    # Test pipeline with different scenarios
    integration_results = []
    
    for i, scenario in enumerate(scenarios[:5], 1):  # Test first 5 scenarios
        print(f"\nüìä Testing Scenario {i}: {scenario.name}")
        
        # Generate study card
        study_card = generator.generate_study_card(scenario)
        
        # Import here to avoid circular imports
        from ncfd.signals import evaluate_all_signals, evaluate_all_gates
        from ncfd.scoring import ScoringEngine
        
        # Run complete pipeline
        try:
            # Evaluate signals
            signals = evaluate_all_signals(study_card)
            fired_signals = [s_id for s_id, s in signals.items() if s.fired]
            
            # Evaluate gates
            gates = evaluate_all_gates(signals)
            fired_gates = [g_id for g_id, g in gates.items() if g.fired]
            
            # Score trial
            engine = ScoringEngine()
            trial_metadata = {
                "trial_id": i,
                "is_pivotal": scenario.trial_type.value in ["pivotal", "phase_3"],
                "indication": scenario.indication.value,
                "phase": scenario.trial_type.value,
                "sponsor_experience": "experienced",
                "primary_endpoint_type": "response"
            }
            score = engine.score_trial(i, trial_metadata, gates, f"integration_test_{i}")
            
            # Check if results match expectations
            expected_signals = set(scenario.expected_signals)
            actual_signals = set(fired_signals)
            signal_match = len(expected_signals & actual_signals) / max(len(expected_signals), 1)
            
            expected_gates = set(scenario.expected_gates)
            actual_gates = set(fired_gates)
            gate_match = len(expected_gates & actual_gates) / max(len(expected_gates), 1) if expected_gates else 1.0
            
            integration_results.append({
                "scenario": scenario.name,
                "success": True,
                "signal_match": signal_match,
                "gate_match": gate_match,
                "prior_pi": score.prior_pi,
                "p_fail": score.p_fail,
                "signals_fired": len(fired_signals),
                "gates_fired": len(fired_gates)
            })
            
            print(f"  ‚úÖ Success: {len(fired_signals)} signals, {len(fired_gates)} gates fired")
            print(f"     Signal match: {signal_match:.1%}, Gate match: {gate_match:.1%}")
            print(f"     Prior: {score.prior_pi:.3f}, Posterior: {score.p_fail:.3f}")
            
        except Exception as e:
            integration_results.append({
                "scenario": scenario.name,
                "success": False,
                "error": str(e)
            })
            print(f"  ‚ùå Failed: {e}")
    
    # Integration summary
    successful_tests = sum(1 for r in integration_results if r["success"])
    total_tests = len(integration_results)
    
    print(f"\nüìà Integration Test Summary:")
    print(f"Successful integrations: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")
    
    if successful_tests > 0:
        avg_signal_match = sum(r.get("signal_match", 0) for r in integration_results if r["success"]) / successful_tests
        avg_gate_match = sum(r.get("gate_match", 0) for r in integration_results if r["success"]) / successful_tests
        print(f"Average signal accuracy: {avg_signal_match:.1%}")
        print(f"Average gate accuracy: {avg_gate_match:.1%}")
    
    return integration_results


def demo_stress_testing():
    """Demonstrate stress testing capabilities."""
    print("\nüí™ DEMO: STRESS TESTING")
    print("=" * 50)
    
    print("üèãÔ∏è  Running stress tests...")
    
    # Test with increasing load
    stress_results = []
    test_sizes = [100, 250, 500]  # Moderate sizes for demo
    
    for size in test_sizes:
        print(f"\nüìä Stress testing with {size} trials...")
        
        # Generate large dataset
        generator = SyntheticDataGenerator(seed=999)
        study_cards = [generator.generate_study_card() for _ in range(size)]
        
        # Time the complete pipeline
        start_time = time.time()
        
        try:
            from ncfd.signals import evaluate_all_signals, evaluate_all_gates
            
            # Process all trials
            for i, study_card in enumerate(study_cards):
                signals = evaluate_all_signals(study_card)
                gates = evaluate_all_gates(signals)
                
                # Every 50 trials, print progress for larger datasets
                if size >= 500 and (i + 1) % 50 == 0:
                    print(f"    Processed {i + 1}/{size} trials...")
            
            end_time = time.time()
            total_time = end_time - start_time
            throughput = size / total_time
            
            stress_results.append({
                "size": size,
                "total_time": total_time,
                "throughput": throughput,
                "success": True
            })
            
            print(f"  ‚úÖ Completed in {total_time:.2f}s ({throughput:.1f} trials/sec)")
            
        except Exception as e:
            stress_results.append({
                "size": size,
                "error": str(e),
                "success": False
            })
            print(f"  ‚ùå Failed: {e}")
    
    # Stress test summary
    successful_tests = [r for r in stress_results if r["success"]]
    
    if successful_tests:
        print(f"\nüìà Stress Test Results:")
        for result in successful_tests:
            print(f"  {result['size']:3d} trials: {result['throughput']:6.1f} trials/sec")
        
        # Check for performance degradation
        if len(successful_tests) >= 2:
            first_throughput = successful_tests[0]["throughput"]
            last_throughput = successful_tests[-1]["throughput"]
            degradation = (first_throughput - last_throughput) / first_throughput * 100
            
            if degradation > 10:
                print(f"  ‚ö†Ô∏è  Performance degradation: {degradation:.1f}%")
            else:
                print(f"  ‚úÖ Good scalability (degradation: {degradation:.1f}%)")
    
    return stress_results


def main():
    """Run the complete testing and validation demo."""
    print("üß™ TRIAL FAILURE DETECTION: TESTING & VALIDATION DEMO")
    print("=" * 65)
    print("This demo showcases the comprehensive testing framework including")
    print("synthetic data generation, performance benchmarks, validation,")
    print("edge case testing, and stress testing capabilities.\n")
    
    start_time = time.time()
    
    try:
        # Run all demo components
        historical_data = demo_synthetic_data_generation()
        demo_performance_benchmarking()
        validation_data = demo_validation_framework()
        edge_case_results = demo_edge_case_testing()
        integration_results = demo_integration_testing()
        stress_results = demo_stress_testing()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nüéâ TESTING & VALIDATION DEMO COMPLETED!")
        print("=" * 50)
        print(f"Total demo time: {total_time:.1f} seconds")
        print()
        
        # Summary statistics
        print("üìä DEMO SUMMARY:")
        print(f"  ‚Ä¢ Generated {len(historical_data)} historical trials for calibration")
        print(f"  ‚Ä¢ Validated system with {len(validation_data)} synthetic trials")
        print(f"  ‚Ä¢ Executed {len(edge_case_results)} edge case tests")
        print(f"  ‚Ä¢ Completed {len(integration_results)} integration tests")
        print(f"  ‚Ä¢ Performed stress testing up to {max(r['size'] for r in stress_results if r['success'])} trials")
        
        print("\nüéØ KEY ACHIEVEMENTS:")
        print("  ‚úÖ Comprehensive synthetic data generation working")
        print("  ‚úÖ Performance benchmarking system operational") 
        print("  ‚úÖ Validation framework successfully validating accuracy")
        print("  ‚úÖ Edge case testing covering boundary conditions")
        print("  ‚úÖ Integration testing confirming end-to-end functionality")
        print("  ‚úÖ Stress testing validating system scalability")
        
        print("\nüìã TESTING FRAMEWORK CAPABILITIES:")
        print("  ‚Ä¢ Synthetic Data: Realistic trial scenarios with known outcomes")
        print("  ‚Ä¢ Performance: Throughput, latency, and memory benchmarks")
        print("  ‚Ä¢ Validation: Accuracy metrics with cross-validation")
        print("  ‚Ä¢ Edge Cases: Robustness testing with extreme conditions")
        print("  ‚Ä¢ Integration: End-to-end pipeline verification")
        print("  ‚Ä¢ Stress Testing: Scalability and performance under load")
        
        print("\nüöÄ SYSTEM STATUS:")
        print("  Phase 5: Testing & Validation - ‚úÖ COMPLETE")
        print("  Ready for: Production deployment and real-world validation")
        
    except Exception as e:
        print(f"\n‚ùå Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
